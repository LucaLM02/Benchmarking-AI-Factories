# Benchmarking-AI-Factories

**EUMaster4HPC Student Challenge 2025-2026**  
**Benchmarking AI Factories on MeluXina Supercomputer**

This repository contains design documents, source code, logs, benchmarking framework, monitoring dashboards, and final reports related to the challenge **"Benchmarking AI Factories on MeluXina"**.  
The goal is to develop a **unified benchmarking framework** capable of evaluating the performance and scalability of different **AI Factory components** (storage, inference, vector databases, orchestration, and monitoring) using the **MeluXina supercomputer**.

---

## üéØ Objectives

- **Design and implement** a modular benchmarking framework for AI Factory workloads.
- **Evaluate and compare** storage, inference, and retrieval systems under realistic HPC-scale conditions.
- **Develop reusable benchmarking tools** with modular architecture (Python + Slurm + Apptainer).
- **Produce performance insights**, dashboards, and recommendations for future AI Factory designs.

---

## üß© Components Tested
The benchmark targets the following AI Factory building blocks:
- **Storage systems** (File systems, Object storage such as MinIO/S3)
- **Relational databases** (PostgreSQL, etc.)
- **Inference servers** (Triton, vLLM)
- **Vector databases** (Chroma, Faiss, Milvus, Weaviate)
- **Monitoring and orchestration** (Prometheus, Grafana, Slurm)

Each component is tested independently and as part of an **end-to-end AI pipeline**, with metrics such as **throughput, latency, scalability, and resource utilization** collected automatically.

---

## ‚öôÔ∏è Methodology

1. Define benchmark **recipes** in YAML format describing:
   - Services and clients (CPU/GPU, nodes, execution mode)
   - Monitors and loggers
   - Execution parameters (warmup, duration, replicas)
   - Reporting and notification settings

2. Run benchmarks on **MeluXina HPC** using **Apptainer containers** orchestrated via **Slurm**.

3. Collect metrics and logs into unified dashboards (Prometheus + Grafana) and generate CSV/PDF reports.

4. Analyze results to provide insights on performance scaling and system bottlenecks.

## üß© UML Design

The following diagram shows the class design of the Benchmarking-AI-Factories framework:

![Benchmark Design Diagram](docs/design.png)

## üöÄ Quick Tutorial ‚Äì How to Run the Benchmark on MeluXina

Follow these steps to reproduce or execute a benchmark.

### 1Ô∏è‚É£ Access MeluXina
Connect to MeluXina using your assigned credentials:
```bash
ssh <your-user-ID>@login.lxp.lu -p 8822 -i ~/.ssh/id_ed25519_mlux

```

### 2Ô∏è‚É£ Allocate a compute node
Request an interactive compute node:
```bash
salloc --nodes=1 --ntasks=1 --time=02:00:00 --mem=32G --partition=cpu

```

### 3Ô∏è‚É£ Load required module
```bash
module add Apptainer

```

### 4Ô∏è‚É£ Clone this repository
```bash
git clone https://github.com/LucaLM02/Benchmarking-AI-Factories.git
cd Benchmarking-AI-Factories

```
### 5Ô∏è‚É£ Build the Apptainer image
```bash
apptainer build benchmark.sif apptainer.def

```
### 6Ô∏è‚É£ Set up your workspace
Define a dynamic workspace for the benchmark:
```bash
export JOB_NAME="job-name"
export WORKSPACE="/scratch/${USER}/benchmarks/${JOB_NAME}_$(date +%Y%m%d_%H%M%S)"
mkdir -p ${WORKSPACE}

```

### 7Ô∏è‚É£ Run the benchmark
```bash
apptainer run \
  --bind $(pwd):/workspace:ro \
  --bind ${WORKSPACE}:/output:rw \
  benchmark.sif \
  --load /workspace/Recipes/Meluxina_DataIngestionRecipe.yaml \
  --workspace /output \
  --run

```



