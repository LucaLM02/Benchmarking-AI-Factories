meta:
  name: "vllm-inference-benchmark-meluxina"
  description: "Benchmark vLLM inference distribuito su nodi HPC Meluxina."
  author: "LucaLM02"
  created_at: "2025-11-01T00:00:00Z"
  commit: "vllm-bootstrap"

global:
  workspace: "${WORKSPACE}"
  timeout: 3600
  environment:
    - BENCH_MODE=single-node
    - HF_HOME=${HF_HOME:-${WORKSPACE}/hf-cache}

# ------------------------------------------------------------------
# SERVICES (server) → Apptainer OR Slurm
# ------------------------------------------------------------------
services:
  - id: "vllm-01"
    role: "inference-server"

    executor:
      type: "process"

    command: |
      export VLLM_MODEL="${VLLM_MODEL:-facebook/opt-125m}"
      export VLLM_PORT="${VLLM_PORT:-8000}"
      export VLLM_TP_SIZE="${VLLM_TP_SIZE:-1}"
      export HF_HOME="${HF_HOME:-${WORKSPACE}/hf-cache}"
      mkdir -p "${HF_HOME}"
      
      # Default to a recent vLLM image compatible with Meluxina
      VLLM_IMAGE_TAG=${VLLM_IMAGE_TAG:-latest}
      
      # Check for pre-built SIF image in project images/ directory
      # This avoids the slow conversion process on the compute node
      PROJECT_IMAGES="${PROJECT_DIR}/images"
      LOCAL_SIF="${PROJECT_IMAGES}/vllm-openai_latest.sif"
      
      if [ -f "${LOCAL_SIF}" ]; then
          echo "Using local SIF image: ${LOCAL_SIF}" > "${WORKSPACE}/vllm_service.log"
          VLLM_IMAGE_URI="${LOCAL_SIF}"
      else
          echo "Local SIF not found at ${LOCAL_SIF}, using Docker URI (slow conversion expected)" > "${WORKSPACE}/vllm_service.log"
          VLLM_IMAGE_URI="docker://vllm/vllm-openai:${VLLM_IMAGE_TAG}"
      fi
      
      # Target path inside container for HuggingFace cache
      CONTAINER_HF_HOME="/tmp/hf-cache"
      
      echo "Starting vLLM Service..." >> "${WORKSPACE}/vllm_service.log"
      echo "Model: ${VLLM_MODEL}" >> "${WORKSPACE}/vllm_service.log"
      
      # Use Apptainer with NV (NVIDIA) support
      # Redirecting output to log file for debugging
      apptainer exec --nv \
        --bind "${HF_HOME}:${CONTAINER_HF_HOME}" \
        --env "HF_HOME=${CONTAINER_HF_HOME}" \
        "${VLLM_IMAGE_URI}" \
          python3 -m vllm.entrypoints.openai.api_server \
          --model "${VLLM_MODEL}" \
          --port "${VLLM_PORT}" \
          --host "0.0.0.0" \
          --tensor-parallel-size "${VLLM_TP_SIZE}" \
          --gpu-memory-utilization 0.9 \
          >> "${WORKSPACE}/vllm_service.log" 2>&1

    healthcheck:
      type: "http"
      url: "http://127.0.0.1:8000/health"
      expect_status: 200
      interval: 10
      timeout: 600  # 10 minutes for model loading

    monitor: "prometheus-vllm"
    logger: "file-compact"

# ------------------------------------------------------------------
# CLIENTS (carico) → Slurm + Apptainer
# ------------------------------------------------------------------
clients:
  - id: "vllm-loadgen"
    type: "vllm-inference"
    instances: 2

    executor:
      type: "workload"

    workload:
      type: "vllm-inference"
      mode: "completion"  # OPT models are completion-only, not chat
      endpoint: "http://127.0.0.1:8000/v1/completions"
      model: "facebook/opt-125m"  # Must match the model loaded by vLLM server
      
      # Conservative parameters to avoid GPU memory saturation
      duration_sec: 300          # 5 minute test
      concurrency: 4             # Reduced from 8 to prevent KV cache exhaustion
      max_tokens: 128            # Reduced from 256 to lower memory pressure
      temperature: 0.7
      timeout: 120               # Per-request timeout (seconds)
      error_backoff_sec: 2.0     # Wait between retries on error
      max_consecutive_errors: 20 # Abort thread after this many failures
      
      prompt: "Explain how GPU memory allocation works in LLM inference."
      label: "vllm-stress-test"

    monitor: "prometheus-vllm"
    logger: "file-compact"

    retries:
      attempts: 3
      backoff: 10

# ------------------------------------------------------------------
# MONITORING
# ------------------------------------------------------------------
monitors:
  - id: "prometheus-vllm"
    type: "prometheus"
    targets: ["127.0.0.1:8000"]
    scrape_interval: 5
    collect_interval: 10
    save_as: "vllm_metrics.json"
    readable_save_as: "vllm_metrics_parsed.json"
    metrics_path: "/metrics"

# ------------------------------------------------------------------
# LOGGING
# ------------------------------------------------------------------
loggers:
  - id: "file-compact"
    type: "file"
    paths:
      - "${global.workspace}/logs"
    file_name: "vllm_benchmark.log"
    format: "json"

# ------------------------------------------------------------------
# EXECUTION
# ------------------------------------------------------------------
execution:
  warmup: 20
  duration: 120
  replicas: 1
  poll_interval: 5
  post_actions:
    - collect_metrics
    - stop_services

# ------------------------------------------------------------------
# REPORTING
# ------------------------------------------------------------------
reporting:
  outputs:
    - type: "csv"
      file: "${global.workspace}/metrics_summary.csv"
    - type: "pdf"
      file: "${global.workspace}/final_report.pdf"

notifications:
  webhook: ""

cleanup:
  remove_containers: true
  preserve_artifacts: false
  paths:
    - "${global.workspace}/hf-cache"
