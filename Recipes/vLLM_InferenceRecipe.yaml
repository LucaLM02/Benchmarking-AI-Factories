meta:
  name: "vllm-inference-benchmark"
  description: "server vLLM"
  author: "LucaLM02"
  created_at: "2025-11-01T00:00:00Z"
  commit: "vllm-bootstrap"

global:
  workspace: "${WORKSPACE}"
  timeout: 3600
  environment:
    - HF_HOME=${HF_HOME:-${WORKSPACE}/hf-cache}

# ------------------------------------------------------------------
# SERVICES
# ------------------------------------------------------------------
services:
  - id: "vllm-01"
    role: "inference-server"

    executor:
      type: "process"

    command: |
      export VLLM_MODEL="${VLLM_MODEL:-facebook/opt-125m}"
      export VLLM_PORT="${VLLM_PORT:-8000}"
      export VLLM_METRICS_PORT="${VLLM_METRICS_PORT:-8001}"
      export VLLM_TP_SIZE="${VLLM_TP_SIZE:-1}"
      export HF_HOME="${HF_HOME:-${WORKSPACE}/hf-cache}"
      mkdir -p "${HF_HOME}"
      python -m vllm.entrypoints.openai.api_server \
        --model "${VLLM_MODEL}" \
        --port "${VLLM_PORT}" \
        --host "0.0.0.0" \
        --tensor-parallel-size "${VLLM_TP_SIZE}" \
        --metrics-port "${VLLM_METRICS_PORT}"

    healthcheck:
      type: "http"
      url: "http://127.0.0.1:8000/health"
      expect_status: 200
      interval: 5
      timeout: 180

    monitor: "prometheus-vllm"
    logger: "file-compact"

# ------------------------------------------------------------------
# CLIENTS
# ------------------------------------------------------------------
clients:
  - id: "vllm-loadgen"
    type: "vllm-inference"
    instances: 2

    executor:
      type: "workload"

    workload:
      type: "vllm-inference"
      mode: "chat"
      endpoint: "http://127.0.0.1:8000/v1/chat/completions"
      requests: 60
      concurrency: 4
      max_tokens: 64
      temperature: 0.1
      prompt: "Give me a one sentence summary of the latest weather in Rome."
      label: "vllm-chat"

    monitor: "prometheus-vllm"
    logger: "file-compact"

    retries:
      attempts: 2
      backoff: 5

# ------------------------------------------------------------------
# MONITORING
# ------------------------------------------------------------------
monitors:
  - id: "prometheus-vllm"
    type: "prometheus"
    targets: ["127.0.0.1:8001"]
    scrape_interval: 5
    collect_interval: 10
    save_as: "vllm_metrics.json"
    readable_save_as: "vllm_metrics_parsed.json"
    metrics_path: "/metrics"

# ------------------------------------------------------------------
# LOGGING
# ------------------------------------------------------------------
loggers:
  - id: "file-compact"
    type: "file"
    paths:
      - "${global.workspace}/logs"
    file_name: "vllm_benchmark.log"
    format: "json"

# ------------------------------------------------------------------
# EXECUTION
# ------------------------------------------------------------------
execution:
  warmup: 20
  duration: 120
  replicas: 1
  poll_interval: 5
  post_actions:
    - collect_metrics
    - stop_services

# ------------------------------------------------------------------
# REPORTING
# ------------------------------------------------------------------
reporting:
  outputs:
    - type: "csv"
      file: "${global.workspace}/metrics_summary.csv"
    - type: "pdf"
      file: "${global.workspace}/final_report.pdf"

notifications:
  webhook: ""

cleanup:
  remove_containers: true
  preserve_artifacts: false
